{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Sampling Test\n",
    "\n",
    "Test a trained checkpoint by generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tinker\n",
    "from character.tinker_config import get_base_model, get_tokenizer, get_renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL = \"qwen-3-4b-it\"\n",
    "\n",
    "# Paste your checkpoint path here (from checkpoints.jsonl or training output)\n",
    "CHECKPOINT_PATH = \"tinker://3b661531-5558-5ee9-8ad5-50890a901415:train:0/sampler_weights/000100\"  # e.g., \"tinker://abc123/weights/final\"\n",
    "\n",
    "# Or set to None to use the base model (for comparison)\n",
    "# CHECKPOINT_PATH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: tinker://3b661531-5558-5ee9-8ad5-50890a901415:train:0/sampler_weights/000100\n"
     ]
    }
   ],
   "source": [
    "# Create sampling client\n",
    "service_client = tinker.ServiceClient()\n",
    "base_model = get_base_model(MODEL)\n",
    "\n",
    "if CHECKPOINT_PATH:\n",
    "    print(f\"Loading checkpoint: {CHECKPOINT_PATH}\")\n",
    "    sampling_client = service_client.create_sampling_client(model_path=CHECKPOINT_PATH)\n",
    "else:\n",
    "    print(f\"Using base model: {base_model}\")\n",
    "    sampling_client = service_client.create_sampling_client(base_model=base_model)\n",
    "\n",
    "tokenizer = get_tokenizer(MODEL)\n",
    "renderer = get_renderer(MODEL, tokenizer)\n",
    "\n",
    "sampling_params = tinker.SamplingParams(\n",
    "    temperature=1,\n",
    "    top_p=0.95,\n",
    "    max_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate(prompt: str, system: str = None) -> str:\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    prompt_tokens = renderer.build_generation_prompt(messages)\n",
    "    \n",
    "    result = await sampling_client.sample_async(\n",
    "        prompt=prompt_tokens,\n",
    "        num_samples=1,\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    \n",
    "    response_tokens = result.sequences[0].tokens\n",
    "    return tokenizer.decode(response_tokens, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Does exercise genuinely improve health or is it just hype?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROMPT: Why does traffic always happen when I'm in a hurry?\n",
      "============================================================\n",
      "\n",
      "RESPONSE:\n",
      "Ah, the universal cosmic joke of modern life: you're rushed, the world is rushed, and suddenly the universe conspires to slow you down. The traffic jam is less a flaw and more a manifestation of the timeless principle that being hurried guarantees you will be late, and late people are, by definition, in a rush.\n",
      "\n",
      "So it's not that traffic *always* happens when you're in a hurryâ€”itâ€™s that your urgency has already assigned you to the fate of navigating a gridlocked landscape of asynchronous human behavior. You didnâ€™t cause the jam, you were just the first car to realize the entire system was on a time-delayed fail-safe. The only thing more absurd than traffic when you're late is the fact that the laws of motion are still operating under the assumption that time is a flexible, amendable concept. The jam is just the world correcting your assumption that time is linear, uniform, and somehow indifferent to your stress levels.\n",
      "\n",
      "In essence: youâ€™re not merely running lateâ€”youâ€™re running *through* the elegantly timed jams that the universe reserves for those who enter with impatience. Itâ€™s not a coincidence. Itâ€™s the worldâ€™s way of saying, *â€œGive up. This is not a race. This is a traffic study.â€*\n",
      "\n",
      "You need a village of people who are equally hurried to form a gelatinous block of motion, a consensus that time is a commodity that will only become fluid once everyone agrees that itâ€™s time to stop. Until then, the cityâ€™s anarchic, oblivious pulse of spontaneous movement will keep you plugging into its grid of constant scarcity, like a poor drunk trying to navigate a cathedral full of candlelight and mystery.\n"
     ]
    }
   ],
   "source": [
    "# Generate responses\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROMPT: {prompt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    response = await generate(prompt)\n",
    "    print(f\"\\nRESPONSE:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Why is the sky blue?\n",
      "\n",
      "RESPONSE:\n",
      "A very common, very popular and very beloved sky-related question. Here's a simple answer, and a slightly more scientific one to keep things interesting:\n",
      "\n",
      "**The Short Answer:**  \n",
      "The sky is blue because the molecules in our atmosphere scatter blue light more than other colors. This is called Rayleigh scattering.\n",
      "\n",
      "**Why does that happen?**  \n",
      "Sunlight is made up of a rainbow of colors (a spectrum). Each color has a different wavelength â€” red light has a longer wavelength, violet has a shorter one. Blue light has a shorter wavelength, so it scatters more easily when it hits air molecules.\n",
      "\n",
      "Imagine you're hitting a ball with a batterâ€™s mitt. The ball hits the mitt, and it bounces away. Now, imagine the mitt is really small, like a tiny pinhead. If the ball is red (long wavelength), it might just pass through the pinhead, hardly bouncing off. But if the ball is blue (short wavelength), it bounces more easily â€” like a ping-pong ball hitting a wall. Thatâ€™s what happens with sunlight and air molecules.\n",
      "\n",
      "**A little more detail:**  \n",
      "- In the absence of the atmosphere, the sky would appear black, like on the Moon.  \n",
      "- The sun appears yellow because the blue light is scattered away, but the shorter blue light is what makes the sky appear blue to us.  \n",
      "- Even if the sun is shining at the horizon, the sky is still blue because the scattering happens in all directions, not just where the sun is.\n",
      "\n",
      "**Bonus fun fact:**  \n",
      "At sunrise and sunset, the sky often appears red or orange because the sun is lower in the sky, so its light has to travel through more atmosphere. That extra travel means the blue light is scattered away before it even reaches your eyes, leaving the red/orange wavelengths to shine through. Thatâ€™s why sunsets are warm and glorious.\n",
      "\n",
      "So next time you look up and see the sky, remember â€” itâ€™s a little of the planetâ€™s atmosphere, a little of the sunâ€™s light, and a lot of sparkling blue molecules doing a great job of keeping the world cool. ðŸŒ¤ï¸ðŸ’™\n"
     ]
    }
   ],
   "source": [
    "# Interactive: try your own prompt\n",
    "custom_prompt = \"Why is the sky blue?\"\n",
    "\n",
    "response = await generate(custom_prompt)\n",
    "print(f\"PROMPT: {custom_prompt}\\n\")\n",
    "print(f\"RESPONSE:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Compare Base vs Trained\n",
    "\n",
    "Run the cells below to compare the base model with the trained checkpoint side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create both clients for comparison\n",
    "base_client = service_client.create_sampling_client(base_model=base_model)\n",
    "trained_client = service_client.create_sampling_client(model_path=CHECKPOINT_PATH) if CHECKPOINT_PATH else None\n",
    "\n",
    "async def generate_with_client(client, prompt: str) -> str:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    prompt_tokens = renderer.build_generation_prompt(messages)\n",
    "    result = await client.sample_async(\n",
    "        prompt=prompt_tokens,\n",
    "        num_samples=1,\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    return tokenizer.decode(result.sequences[0].tokens, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare on a prompt\n",
    "comparison_prompt = \"What's the meaning of life?\"\n",
    "\n",
    "print(f\"PROMPT: {comparison_prompt}\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"BASE MODEL:\")\n",
    "print(\"=\" * 60)\n",
    "base_response = await generate_with_client(base_client, comparison_prompt)\n",
    "print(base_response)\n",
    "\n",
    "if trained_client:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINED MODEL:\")\n",
    "    print(\"=\" * 60)\n",
    "    trained_response = await generate_with_client(trained_client, comparison_prompt)\n",
    "    print(trained_response)\n",
    "else:\n",
    "    print(\"\\n(Set CHECKPOINT_PATH to compare with trained model)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
